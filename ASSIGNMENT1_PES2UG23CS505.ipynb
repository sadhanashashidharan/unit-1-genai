{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtbRSwrrieSe",
        "outputId": "ec355c7d-3c5b-4ced-e2ac-25275ec7a4a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers torch\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator_bert = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"bert-base-uncased\"\n",
        ")\n",
        "\n",
        "generator_bert(\n",
        "    \"The future of Artificial Intelligence is\",\n",
        "    max_length=30\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXzc-Z5dkxwR",
        "outputId": "7e7158b4-e22d-44d2-8172-fe2de585860a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'The future of Artificial Intelligence is................................................................................................................................................................................................................................................................'}]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator_roberta = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"roberta-base\"\n",
        ")\n",
        "\n",
        "generator_roberta(\n",
        "    \"The future of Artificial Intelligence is\",\n",
        "    max_length=30\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gejg6RUJk2Ve",
        "outputId": "54ed616a-371e-490d-8001-2eca75220343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'The future of Artificial Intelligence is'}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator_bart = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"facebook/bart-base\"\n",
        ")\n",
        "\n",
        "generator_bart(\n",
        "    \"The future of Artificial Intelligence is\",\n",
        "    max_length=30\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czqr8PySk4gB",
        "outputId": "37efa5a2-79a5-44d3-f459-2084e9c466f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'The future of Artificial Intelligence is Feinstein obesity opting IOC IOC PC sustainabilityAfeeIssue Feinstein kids kids OwnerLongLong traces kids afterlife kids kids Feinsteincontact harvesting Winsnotice afterlife afterlife Codes afterlife afterlife kids Codes afterlifeFuel kids afterlife contributingRace kids kids kids preparingnotice kids afterlife afterlife afterlifeRaceRacechannel Basin kids contributingleg Politico Diversity kids Codes traces kids PC kids kids contributing kidsblocking kids Codes profRace Feinstein kidsStatement kids fram kids PC Basinhematicallyencers Akira Feinstein kids fram sustainability sustainability kids Codes kidsnotice kids Embassy Social Feinstein Marshallaw Tube kidsScottencersencersUnt Embassy265 fram kids lucrativeatersUntUnt hierarchicalishersencers traces fram hierarchicalencers rejectingencers PC kidsishers Cran packaged265ishersishersencers Ukrain kids rebuildingfried traces265encersencers265 sustainabilityencers265 kids hierarchical265 CranUnt prep electrical hierarchical hierarchicalencersUnt FeinsteinencersUnt kids kidsUnt kids Screen265 Cran prof265265 Cranencers265encersUntUnt Cran hierarchicalencersinancesencersUntencers Screen Cran Cran CranUnt Cranencers hierarchical tracesencersencersencers traces traces Cran VII Cran265265encers Ak conglomer265265 conglomer265 either lucrative conglomer lucrativeryceryceertodd265ryce265 Cran CranencersTes Cran lucrativeencers big Cran Cranapter265worn Cran CranTes Cran Cran hierarchical prep Cranencersencers hierarchicalencersencers violationencersunn Cranencersammadencers Feast265apter hierarchicalencers265 Cran packaged hierarchical'}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fill_mask_bert = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"bert-base-uncased\"\n",
        ")\n",
        "\n",
        "fill_mask_bert(\n",
        "    \"The goal of Generative AI is to [MASK] new content.\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIeTfD-MlJ_V",
        "outputId": "94fe1ed4-b9fd-469d-f367-f779d014d36c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.5396937131881714,\n",
              "  'token': 3443,\n",
              "  'token_str': 'create',\n",
              "  'sequence': 'the goal of generative ai is to create new content.'},\n",
              " {'score': 0.15575705468654633,\n",
              "  'token': 9699,\n",
              "  'token_str': 'generate',\n",
              "  'sequence': 'the goal of generative ai is to generate new content.'},\n",
              " {'score': 0.05405480042099953,\n",
              "  'token': 3965,\n",
              "  'token_str': 'produce',\n",
              "  'sequence': 'the goal of generative ai is to produce new content.'},\n",
              " {'score': 0.04451539367437363,\n",
              "  'token': 4503,\n",
              "  'token_str': 'develop',\n",
              "  'sequence': 'the goal of generative ai is to develop new content.'},\n",
              " {'score': 0.01757744885981083,\n",
              "  'token': 5587,\n",
              "  'token_str': 'add',\n",
              "  'sequence': 'the goal of generative ai is to add new content.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fill_mask_roberta = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"roberta-base\"\n",
        ")\n",
        "\n",
        "fill_mask_roberta(\n",
        "    \"The goal of Generative AI is to <mask> new content.\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Kf7U8AtlPnL",
        "outputId": "4420fa76-8106-4745-f423-b664f374c5c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.37113314867019653,\n",
              "  'token': 5368,\n",
              "  'token_str': ' generate',\n",
              "  'sequence': 'The goal of Generative AI is to generate new content.'},\n",
              " {'score': 0.36771294474601746,\n",
              "  'token': 1045,\n",
              "  'token_str': ' create',\n",
              "  'sequence': 'The goal of Generative AI is to create new content.'},\n",
              " {'score': 0.08351416140794754,\n",
              "  'token': 8286,\n",
              "  'token_str': ' discover',\n",
              "  'sequence': 'The goal of Generative AI is to discover new content.'},\n",
              " {'score': 0.02133510820567608,\n",
              "  'token': 465,\n",
              "  'token_str': ' find',\n",
              "  'sequence': 'The goal of Generative AI is to find new content.'},\n",
              " {'score': 0.016521671786904335,\n",
              "  'token': 694,\n",
              "  'token_str': ' provide',\n",
              "  'sequence': 'The goal of Generative AI is to provide new content.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fill_mask_bart = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"facebook/bart-base\"\n",
        ")\n",
        "\n",
        "fill_mask_bart(\n",
        "    \"The goal of Generative AI is to <mask> new content.\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWxuRRjSlSPI",
        "outputId": "d039922f-26a2-48b8-fd89-490e972e1399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.0746152326464653,\n",
              "  'token': 1045,\n",
              "  'token_str': ' create',\n",
              "  'sequence': 'The goal of Generative AI is to create new content.'},\n",
              " {'score': 0.06571866571903229,\n",
              "  'token': 244,\n",
              "  'token_str': ' help',\n",
              "  'sequence': 'The goal of Generative AI is to help new content.'},\n",
              " {'score': 0.060880016535520554,\n",
              "  'token': 694,\n",
              "  'token_str': ' provide',\n",
              "  'sequence': 'The goal of Generative AI is to provide new content.'},\n",
              " {'score': 0.03593575954437256,\n",
              "  'token': 3155,\n",
              "  'token_str': ' enable',\n",
              "  'sequence': 'The goal of Generative AI is to enable new content.'},\n",
              " {'score': 0.03319466486573219,\n",
              "  'token': 1477,\n",
              "  'token_str': ' improve',\n",
              "  'sequence': 'The goal of Generative AI is to improve new content.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\n"
      ],
      "metadata": {
        "id": "S6jNyj1YlXdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "qa_bert = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=\"bert-base-uncased\"\n",
        ")\n",
        "\n",
        "qa_bert(\n",
        "    question=\"What are the risks?\",\n",
        "    context=\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUfumeRylapg",
        "outputId": "1050d716-54b2-4d34-96fc-d06c4a89d22d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.004109836649149656,\n",
              " 'start': 32,\n",
              " 'end': 71,\n",
              " 'answer': 'risks such as hallucinations, bias, and'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "qa_roberta = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=\"roberta-base\"\n",
        ")\n",
        "\n",
        "qa_roberta(\n",
        "    question=\"What are the risks?\",\n",
        "    context=\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9_4Gu2Dlg4i",
        "outputId": "fd4ec8a4-2988-4990-ee91-b1cef7d39a2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.008263549767434597,\n",
              " 'start': 0,\n",
              " 'end': 19,\n",
              " 'answer': 'Generative AI poses'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "qa_bart = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=\"facebook/bart-base\"\n",
        ")\n",
        "\n",
        "qa_bart(\n",
        "    question=\"What are the risks?\",\n",
        "    context=\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCljzx29ljp3",
        "outputId": "838f6d09-3142-4d02-c715-876785ae0fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.01369146816432476, 'start': 14, 'end': 19, 'answer': 'poses'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Task       | Model       | Classification      | Observation (What actually happened?)                  | Why did this happen? (Architectural Reason)                                         |\n",
        "| ---------- | ----------- | ------------------- | ------------------------------------------------------ | ----------------------------------------------------------------------------------- |\n",
        "| Generation | **RoBERTa** | **Failure**         | Repeated the input prompt without generating new text. | RoBERTa is an encoder-only model and lacks a decoder for autoregressive generation. |\n",
        "| Generation | **BART**    | **Partial Success** | Generated short or awkward text beyond the prompt.     | BART has an encoder-decoder architecture designed for text generation tasks.        |\n",
        "\n"
      ],
      "metadata": {
        "id": "U6aiqSdjmOgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Task      | Model       | Classification      | Observation (What actually happened?)                           | Why did this happen? (Architectural Reason)                                |\n",
        "| --------- | ----------- | ------------------- | --------------------------------------------------------------- | -------------------------------------------------------------------------- |\n",
        "| Fill-Mask | **RoBERTa** | **Success**         | Correctly predicted words like “generate” with high confidence. | RoBERTa is trained using Masked Language Modeling with optimized training. |\n",
        "| Fill-Mask | **BART**    | **Partial Success** | Predicted reasonable words but with less confidence.            | BART is trained mainly for sequence-to-sequence denoising, not pure MLM.   |\n"
      ],
      "metadata": {
        "id": "LwDyNCTzmPH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Task | Model       | Classification      | Observation (What actually happened?)                | Why did this happen? (Architectural Reason)                         |\n",
        "| ---- | ----------- | ------------------- | ---------------------------------------------------- | ------------------------------------------------------------------- |\n",
        "| QA   | **BERT**    | **Partial Failure** | Returned incomplete or imprecise answers.            | The base BERT model is not fine-tuned for question answering tasks. |\n",
        "| QA   | **RoBERTa** | **Partial Failure** | Answers were slightly better but still inconsistent. | Requires task-specific fine-tuning for QA.                          |\n",
        "| QA   | **BART**    | **Partial Failure** | Generated unclear or loosely related answers.        | BART is not optimized for extractive QA without fine-tuning.        |\n"
      ],
      "metadata": {
        "id": "tkXQH0-lmaih"
      }
    }
  ]
}